{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a89b7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.ipynb\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "from PIL import Image\n",
    "import random\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2872f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f97471bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.mobilenet_v2(pretrained=True, progress=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "X, y = shap.datasets.imagenet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "decb11e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ImageNet classes: 1000\n"
     ]
    }
   ],
   "source": [
    "# Getting ImageNet 1000 class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "with open(shap.datasets.cache(url)) as file:\n",
    "    class_names = [v[1] for v in json.load(file).values()]\n",
    "print(\"Number of ImageNet classes:\", len(class_names))\n",
    "#print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81ed0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data transformation pipeline\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        x = x if x.shape[1] == 3 else x.permute(0, 3, 1, 2)\n",
    "    elif x.dim() == 3:\n",
    "        x = x if x.shape[0] == 3 else x.permute(2, 0, 1)\n",
    "    return x\n",
    "\n",
    "def nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        x = x if x.shape[3] == 3 else x.permute(0, 2, 3, 1)\n",
    "    elif x.dim() == 3:\n",
    "        x = x if x.shape[2] == 3 else x.permute(1, 2, 0)\n",
    "    return x \n",
    "        \n",
    "\n",
    "transform= [\n",
    "    torchvision.transforms.Lambda(nhwc_to_nchw),\n",
    "    torchvision.transforms.Lambda(lambda x: x*(1/255)),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std),\n",
    "    torchvision.transforms.Lambda(nchw_to_nhwc),\n",
    "]\n",
    "\n",
    "inv_transform= [\n",
    "    torchvision.transforms.Lambda(nhwc_to_nchw),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean = (-1 * np.array(mean) / np.array(std)).tolist(),\n",
    "        std = (1 / np.array(std)).tolist()\n",
    "    ),\n",
    "    torchvision.transforms.Lambda(nchw_to_nhwc),\n",
    "]\n",
    "\n",
    "transform = torchvision.transforms.Compose(transform)\n",
    "inv_transform = torchvision.transforms.Compose(inv_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2951e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img: np.ndarray) -> torch.Tensor:\n",
    "    img = nhwc_to_nchw(torch.Tensor(img))\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca02c46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [132 814]: ['American_egret' 'speedboat']\n"
     ]
    }
   ],
   "source": [
    "# Check that transformations work correctly\n",
    "Xtr = transform(torch.Tensor(X))\n",
    "out = predict(Xtr[1:3])\n",
    "classes = torch.argmax(out, axis=1).cpu().numpy()\n",
    "print(f'Classes: {classes}: {np.array(class_names)[classes]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9123d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Xtr = Xtr.detach().numpy()\n",
    "print(type(Xtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7699575a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# create an explainer with model and image masker\u001b[39;00m\n\u001b[1;32m      9\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(predict, masker_blur, output_names\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[0;32m---> 11\u001b[0m Xtr \u001b[38;5;241m=\u001b[39m \u001b[43mXtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# feed only one image\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\u001b[39;00m\n\u001b[1;32m     15\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer(Xtr[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m], max_evals\u001b[38;5;241m=\u001b[39mn_evals, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     16\u001b[0m                         outputs\u001b[38;5;241m=\u001b[39mshap\u001b[38;5;241m.\u001b[39mExplanation\u001b[38;5;241m.\u001b[39margsort\u001b[38;5;241m.\u001b[39mflip[:topk])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "source": [
    "topk = 4\n",
    "batch_size = 50\n",
    "n_evals = 1000\n",
    "\n",
    "# define a masker that is used to mask out partitions of the input image.\n",
    "masker_blur = shap.maskers.Image(\"blur(128,128)\", Xtr[0].shape)\n",
    "\n",
    "# create an explainer with model and image masker\n",
    "explainer = shap.Explainer(predict, masker_blur, output_names=class_names)\n",
    "\n",
    "Xtr = Xtr.requires_grad()\n",
    "\n",
    "# feed only one image\n",
    "# here we explain two images using 100 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(Xtr[1:3], max_evals=n_evals, batch_size=batch_size,\n",
    "                        outputs=shap.Explanation.argsort.flip[:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10363434",
   "metadata": {},
   "outputs": [],
   "source": [
    "(shap_values.data.shape, shap_values.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6069f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.data = inv_transform(shap_values.data).cpu().numpy()\n",
    "shap_values.values = [val for val in np.moveaxis(shap_values.values,-1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f264c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(shap_values.data.shape, shap_values.values[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values=shap_values.values,\n",
    "                pixel_values=shap_values.data,\n",
    "                labels=shap_values.output_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
