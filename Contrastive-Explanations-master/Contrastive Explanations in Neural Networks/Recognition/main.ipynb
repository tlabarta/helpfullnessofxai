{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119f6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an implementation of Contrastive Explanations in Neural Networks.\n",
    "# The code is built on top of Grad-CAM explanations. The base implementation of Grad-CAM is derived from :\n",
    "# https://github.com/1Konny/gradcam_plus_plus-pytorch\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from utils import visualize_cam, Normalize\n",
    "from gradcam import GradCAM, Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79daf203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\anaconda3\\envs\\xai_env\\lib\\site-packages\\torch\\nn\\functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "C:\\Users\\julia\\anaconda3\\envs\\xai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saliency_map size : torch.Size([14, 14])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7528\\2496147959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_gradcam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mmask_contrast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_contrast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormed_torch_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m130\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Your choice of contrast; The Q in `Why P, rather than Q?'. Class 130 is flamingo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[0mheatmap_contrast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_contrast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize_cam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_contrast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Dokumente\\GitHub\\development\\Contrastive-Explanations-master\\Contrastive Explanations in Neural Networks\\Recognition\\gradcam.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input, class_idx, retain_graph)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Dokumente\\GitHub\\development\\Contrastive-Explanations-master\\Contrastive Explanations in Neural Networks\\Recognition\\gradcam.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, Q, retain_graph)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mim_label_as_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mpred_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim_label_as_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_arch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xai_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xai_env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2996\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "img_dir = 'Images'\n",
    "img_name = 'water-bird.JPEG'\n",
    "#img_name = 'Flamingo6.jpg'\n",
    "#img_name = 'cat_dog.png'\n",
    "\n",
    "# selbst\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "img_path = os.path.join(img_dir, img_name)\n",
    "\n",
    "pil_img = PIL.Image.open(img_path)\n",
    "\n",
    "#plt.imshow(pil_img)\n",
    "#plt.show()\n",
    "\n",
    "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "torch_img = torch.from_numpy(np.asarray(pil_img)).permute(2, 0, 1).unsqueeze(0).float().div(255) # .cuda()\n",
    "torch_img = F.upsample(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "normed_torch_img = normalizer(torch_img)\n",
    "\n",
    "\n",
    "output_dir = 'Results'\n",
    "folder1 = 'GradCam'\n",
    "folder2 = 'Contrast'\n",
    "\n",
    "os.makedirs(output_dir + '/' + folder1 + '/', exist_ok=True)\n",
    "os.makedirs(output_dir + '/' + folder2 + '/', exist_ok=True)\n",
    "\n",
    "cam_dict = dict()\n",
    "\n",
    "# Please select any of the networks. Your own networks and data can also be used. In that case, the layer specifications\n",
    "# should be provided below (layer_name), and extraction should be done in utils.py\n",
    "# As an example we demo on VGG-16, layer 29\n",
    "\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "vgg_model_dict = dict(type='vgg', arch=vgg, layer_name='features_29', input_size=(224, 224))\n",
    "vgg.eval()#, vgg.cuda()\n",
    "\n",
    "# Example configurations of other architectures are shown below\n",
    "\n",
    "#resnet_model_dict = dict(type='resnet', arch=model, layer_name='layer4', input_size=(224, 224))\n",
    "#alexnet_model_dict = dict(type='alexnet', arch=model, layer_name='features_11', input_size=(224, 224))\n",
    "#densenet_model_dict = dict(type='densenet', arch=model, layer_name='features_norm5', input_size=(224, 224))\n",
    "#squeezenet_model_dict = dict(type='squeezenet', arch=squeezenet, layer_name='features_12_expand3x3_activation', input_size=(224, 224))\n",
    "\n",
    "vgg_gradcam = GradCAM(vgg_model_dict, False)\n",
    "vgg_contrast = Contrast(vgg_model_dict, True)\n",
    "\n",
    "mask_gradcam, logit = vgg_gradcam(normed_torch_img)\n",
    "heatmap_gradcam, result_gradcam = visualize_cam(mask_gradcam, torch_img)\n",
    "\n",
    "output_path = os.path.join(output_dir + '/' + folder1 + '/' + img_name)\n",
    "save_image(result_gradcam, output_path)\n",
    "\n",
    "mask_contrast, _ = vgg_contrast(normed_torch_img, 130)  # Your choice of contrast; The Q in `Why P, rather than Q?'. Class 130 is flamingo\n",
    "heatmap_contrast, result_contrast = visualize_cam(mask_contrast, torch_img)\n",
    "\n",
    "output_path = os.path.join(output_dir + '/' + folder2 + '/' + img_name)\n",
    "save_image(result_contrast, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed352517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77cd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
