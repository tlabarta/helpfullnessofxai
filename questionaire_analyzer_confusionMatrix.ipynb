{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9ef7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from methods import data_handler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from itertools import chain, product\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d5f67",
   "metadata": {},
   "source": [
    "## Read questionnaire information from picking procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3b0da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: .\\data/imagenet_class_index.json\n"
     ]
    }
   ],
   "source": [
    "questionnaires = data_handler.get_questionaires(\"data2/questionaires_shuffled.pickle\")\n",
    "labels = data_handler.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6640676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_img_idx(img_idx, testset_path, labels):    \n",
    "img_folder = datasets.ImageFolder(root=testset_path)\n",
    "img_path = img_folder.imgs[img_idx][0]\n",
    "img_name = img_path.split(os.sep)[-1]\n",
    "    # extract correct class\n",
    "    class_idx_true_str = img_path.split(os.sep)[-2]\n",
    "    img_label_true = labels[class_idx_true_str][1]\n",
    "    return img_label_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9a876",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [00:36<00:07,  3.68s/it]"
     ]
    }
   ],
   "source": [
    "# enrich questionnaire data with image label names\n",
    "questionnaires_2 = []\n",
    "for questionnaire in tqdm(questionnaires):\n",
    "    questionnaire_2 = []\n",
    "    for question in questionnaire:\n",
    "        label = get_label_from_img_idx(question[0], \"data2/imagenetv2-matched-frequency-format-val\", labels)\n",
    "        question_labled = (label, ) + question        \n",
    "        questionnaire_2.append(question_labled)\n",
    "    questionnaires_2.append(questionnaire_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be42d6",
   "metadata": {},
   "source": [
    "## Load question codes used in SoSci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700944c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "codes_list = []\n",
    "for i in range(1, 12+1):\n",
    "    codes = pd.read_csv(f\"questionaires_shuffle_order/questionaire_{i}.txt\", sep=\";\", names=[0, 1])[1]\n",
    "    codes = codes.str.extract(\"(\\w\\d{3})\")\n",
    "    codes = list(codes[0])\n",
    "    codes_list.append(codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee37a8",
   "metadata": {},
   "source": [
    "## Create questions meta data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5444db",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_quest_meta = pd.DataFrame(list(chain(*questionnaires_2)))\n",
    "df_quest_meta[5] = list(chain(*codes_list))\n",
    "df_quest_meta.columns = [\"label\", \"img_idx\", \"model\", \"method\", \"is_pred_correct\", \"question_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf8b97",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_quest_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ca4bb",
   "metadata": {},
   "source": [
    "## Load and transform questionnaire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627e2ff",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# data_tu-helpfulness-of-xai_2022-06-21_17-15.xlsx\n",
    "df = pd.read_excel(\"data2/data_tu-helpfulness-of-xai_2022-06-29_10-48.xlsx\")\n",
    "# delete column descriptions\n",
    "df = df.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221b4fc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_q_data_from_wide_to_long(df):\n",
    "    df_long = pd.melt(df, id_vars=\"CASE\", value_vars=df.columns.values[6:294]).dropna()\n",
    "    df_long.columns = [\"case\", \"question_code\", \"response\"]\n",
    "    df_long = df_long.sort_values(\"case\", )\n",
    "    # map response 1(Yes)/2(No) values to True/False\n",
    "    df_long[\"response\"] = df_long[\"response\"].apply(lambda x: True if x==1 else False) \n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82dfdc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_long = convert_q_data_from_wide_to_long(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374525b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078ef35",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25f0e8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_merged = df_long.merge(right=df_quest_meta)\n",
    "df_merged = df_merged.sort_values(\"case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c58b6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de567e7",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e01e8",
   "metadata": {},
   "source": [
    "### Overall all confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51274ee",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "TP = df_merged[(df_merged[\"is_pred_correct\"] == True) & (df_merged[\"response\"] == True)]\n",
    "TN = df_merged[(df_merged[\"is_pred_correct\"] == False) & (df_merged[\"response\"] == False)]\n",
    "FN = df_merged[(df_merged[\"is_pred_correct\"] == True) & (df_merged[\"response\"] == False)]\n",
    "FP = df_merged[(df_merged[\"is_pred_correct\"] == False) & (df_merged[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc6ea8",
   "metadata": {},
   "source": [
    "### Model wise confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b41eef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c4bc0",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"model\"]==\"vgg\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d109a",
   "metadata": {},
   "source": [
    "#### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f03b76",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"model\"]==\"alex\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3fe9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Method wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66951ae2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53a64d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"method\"]==\"LRP\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275bf16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6dfa9a",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"method\"]==\"gradCAM\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd480a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44284be6",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"method\"]==\"LIME\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3cd74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea770f",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"method\"]==\"SHAP\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48698abf",
   "metadata": {},
   "source": [
    "#### ConfidenceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473a136",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"method\"]==\"ConfidenceScores\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737730a4",
   "metadata": {},
   "source": [
    "#### IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba05b1c",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"method\"]==\"IntegratedGradients\"]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb818f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Questionaire wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ee153",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106b59e",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B1\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458f30e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf78394",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B2\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842a50d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801545bb",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B3\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be527c41",
   "metadata": {},
   "source": [
    "#### Questionaire 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7af420",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B4\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9c474",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e27ca",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B5\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e23e10",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be690c",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B6\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27962d2e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876463ae",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B7\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a5283",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Questionaire 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cab9c7",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B8\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeb82f6",
   "metadata": {},
   "source": [
    "#### Questionaire 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084adb0c",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"B9\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06851fd",
   "metadata": {},
   "source": [
    "#### Questionaire 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cdbc4",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"C1\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24690a",
   "metadata": {},
   "source": [
    "#### Questionaire 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54115271",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"C2\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc5f37",
   "metadata": {},
   "source": [
    "#### Questionaire 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45612305",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged[df_merged[\"question_code\"].apply(lambda x: \"C3\" in x)]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622e8df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fixed Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fecf6",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fixed_img_idxs = df_quest_meta[\"img_idx\"].value_counts()[df_quest_meta[\"img_idx\"].value_counts() == 12].index\n",
    "df_quest_meta_fixed = df_quest_meta[df_quest_meta[\"img_idx\"].isin(fixed_img_idxs)]\n",
    "df_merged_fixed = df_long.merge(right=df_quest_meta_fixed)\n",
    "df_merged_fixed = df_merged_fixed.sort_values(\"case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d741e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### All fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54740690",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp = df_merged_fixed[df_merged_fixed[\"question_code\"].apply(lambda x: helper(x))]\n",
    "TP = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == True)]\n",
    "TN = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == False)]\n",
    "FN = tmp[(tmp[\"is_pred_correct\"] == True) & (tmp[\"response\"] == False)]\n",
    "FP = tmp[(tmp[\"is_pred_correct\"] == False) & (tmp[\"response\"] == True)]\n",
    "\n",
    "cm = np.array([[len(TP),len(FN)],[len(FP),len(TN)]])\n",
    "cm = pd.DataFrame(cm, index=[\"model predicted True\", \"model predicted False\"],\n",
    "                  columns=[\"user predicted True\", \"user predicted False\"])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16},cbar=False,fmt=\"g\") # font size\n",
    "\n",
    "plt.show()\n",
    "acc = (len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))\n",
    "print(\"Accuracy is\",str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb112896",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79251c86",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO do we want to go through all 12 images ?\n",
    "#Images codes for each questionaire\n",
    "coral = [1,4,2,12,1,9,9,10,2,2,8,2]\n",
    "boathouse =[2,6,11,11,5,3,6,9,10,8,2,5]\n",
    "water = [3,10,1,107,5,1,4,1,9,5,6]\n",
    "goose = [4,7,7,6,11,11,8,6,4,5,3,3]\n",
    "nautilus =[5,11,3,1,6,7,11,3,3,4,1,1]\n",
    "rule = [6,8,8,4,9,4,7,12,6,7,12,11]\n",
    "cart =[7,1,10,3,4,12,10,11,5,10,9,10]\n",
    "apple = [8,5,9,9,3,2,12,1,7,3,11,7]\n",
    "gyromitra =[9,3,4,7,12,1,3,2,8,11,6,8]\n",
    "jaguar = [10,12,6,8,8,5,7,11,1,4,4]\n",
    "bull = [11,9,5,5,10,10,4,8,12,6,10,9]\n",
    "stick = [12,2,12,2,6,2,5,9,12,7,12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
